{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tokenize\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILEPATH='./raw/scikit-learn/sklearn/decomposition/_factor_analysis.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_list = []\n",
    "with open(FILEPATH, 'rb') as f:\n",
    "    for tok in tokenize.tokenize(f.readline):\n",
    "        if tok.type == 3:\n",
    "            comment_list.append((tok.start[0], tok.end[0], tok.string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./index/scikit-learn/sklearn/decomposition/_factor_analysis.json', 'r') as f:\n",
    "    file_content = json.loads(f.read())\n",
    "ln_fdef = {}\n",
    "function_params = {}\n",
    "for fd in file_content['FunctionDef']:\n",
    "    for ln in file_content['FunctionDef'][fd]['lineno']:\n",
    "        if ln not in ln_fdef:\n",
    "            ln_fdef[ln] = []\n",
    "        ln_fdef[ln].append(fd)\n",
    "    function_params[fd] = file_content['FunctionDef'][fd]['params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfunc_pairs = []\n",
    "for clns, clne, cs in comment_list:\n",
    "    if clns-1 in ln_fdef:\n",
    "        for f in ln_fdef[clns-1]:\n",
    "            cfunc_pairs.append((f, cs))\n",
    "    if clne+1 in ln_fdef:\n",
    "        for f in ln_fdef[clne+1]:\n",
    "            cfunc_pairs.append((f, cs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['self', 'X', 'y']\n",
      "\"\"\"Fit the FactorAnalysis model to X using SVD based approach.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        X : array-like of shape (n_samples, n_features)\n",
      "            Training data.\n",
      "\n",
      "        y : Ignored\n",
      "            Ignored parameter.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        self : object\n",
      "            FactorAnalysis class instance.\n",
      "        \"\"\"\n",
      "['self', 'X']\n",
      "\"\"\"Apply dimensionality reduction to X using the model.\n",
      "\n",
      "        Compute the expected mean of the latent variables.\n",
      "        See Barber, 21.2.33 (or Bishop, 12.66).\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        X : array-like of shape (n_samples, n_features)\n",
      "            Training data.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        X_new : ndarray of shape (n_samples, n_components)\n",
      "            The latent variables of X.\n",
      "        \"\"\"\n",
      "['self']\n",
      "\"\"\"Compute data covariance with the FactorAnalysis model.\n",
      "\n",
      "        ``cov = components_.T * components_ + diag(noise_variance)``\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        cov : ndarray of shape (n_features, n_features)\n",
      "            Estimated covariance of data.\n",
      "        \"\"\"\n",
      "['self']\n",
      "\"\"\"Compute data precision matrix with the FactorAnalysis model.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        precision : ndarray of shape (n_features, n_features)\n",
      "            Estimated precision of data.\n",
      "        \"\"\"\n",
      "['self', 'X']\n",
      "\"\"\"Compute the log-likelihood of each sample.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        X : ndarray of shape (n_samples, n_features)\n",
      "            The data.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        ll : ndarray of shape (n_samples,)\n",
      "            Log-likelihood of each sample under the current model.\n",
      "        \"\"\"\n",
      "['self', 'X', 'y']\n",
      "\"\"\"Compute the average log-likelihood of the samples.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        X : ndarray of shape (n_samples, n_features)\n",
      "            The data.\n",
      "\n",
      "        y : Ignored\n",
      "            Ignored parameter.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        ll : float\n",
      "            Average log-likelihood of the samples under the current model.\n",
      "        \"\"\"\n",
      "['self', 'components', 'n_components', 'tol']\n",
      "\"Rotate the factor analysis solution.\"\n",
      "['self']\n",
      "\"\"\"Number of transformed output features.\"\"\"\n",
      "['components', 'method', 'tol', 'max_iter']\n",
      "\"\"\"Return rotated components.\"\"\"\n"
     ]
    }
   ],
   "source": [
    "# Provides comment-function pairs\n",
    "for func, comment in cfunc_pairs:\n",
    "    print(function_params[func])\n",
    "    print(comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(os.path.join('./index/', \"file_key.txt\")) as f:\n",
    "#         j = json.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_parameters_from_func_description(func_description):\n",
    "    returns_loc = func_description.find('Returns')\n",
    "    if returns_loc != -1:\n",
    "        func_description = func_description[:returns_loc]\n",
    "    \n",
    "    parameters = {}\n",
    "    line_split_description = func_description.split('\\n')\n",
    "    for line_no, x in enumerate(line_split_description):\n",
    "        if ' : ' in x:\n",
    "            cur_index = line_no + 1\n",
    "            \n",
    "            while cur_index < len(line_split_description) and line_split_description[cur_index].strip() != '' and ':' not in line_split_description[cur_index].strip():\n",
    "                cur_index += 1\n",
    "                \n",
    "            parameter_name = x.split(':')[0].strip()\n",
    "            parameter_description = ' '.join(line_split_description[line_no:cur_index])\n",
    "            parameter_description = ''.join(parameter_description.split(':')[1:]).strip()\n",
    "            parameter_description = parameter_description.replace('\\\\', '').replace('\\r', '').replace('\\t', '').replace(',', '').strip()\n",
    "            parameter_description = ' '.join(parameter_description.split())\n",
    "                \n",
    "            parameters[parameter_name] = parameter_description\n",
    "            \n",
    "    return parameters\n",
    "        \n",
    "\n",
    "def get_parameter_definition_locations(json_filepath, function_name):\n",
    "    DEBUG = True\n",
    "    \n",
    "    raw_filepath = json_filepath.replace('.json', '.py')\n",
    "    comment_list = []\n",
    "    with open(os.path.join('./raw', raw_filepath), 'rb') as f:\n",
    "        for tok in tokenize.tokenize(f.readline):\n",
    "            if tok.type == 3:\n",
    "                comment_list.append((tok.start[0], tok.end[0], tok.string))\n",
    "                \n",
    "    with open(os.path.join('./index', json_filepath), 'r') as f:\n",
    "        file_content = json.loads(f.read())\n",
    "    ln_fdef = {}\n",
    "    function_params = {}\n",
    "    for fd in file_content['FunctionDef']:\n",
    "        for ln in file_content['FunctionDef'][fd]['lineno']:\n",
    "            if ln not in ln_fdef:\n",
    "                ln_fdef[ln] = []\n",
    "            ln_fdef[ln].append(fd)\n",
    "        function_params[fd] = file_content['FunctionDef'][fd]['params']\n",
    "        \n",
    "    cfunc_pairs = {}\n",
    "    for clns, clne, cs in comment_list:\n",
    "        if clns-1 in ln_fdef:\n",
    "            for f in ln_fdef[clns-1]:\n",
    "                cfunc_pairs[f] = cs\n",
    "        if clne+1 in ln_fdef:\n",
    "            for f in ln_fdef[clne+1]:\n",
    "                cfunc_pairs[f] = cs\n",
    "                \n",
    "    if DEBUG:\n",
    "        for func, comment in cfunc_pairs.items():\n",
    "            print(func)\n",
    "            print(function_params[func])\n",
    "            print(comment)\n",
    "        print('\\n')\n",
    "    \n",
    "    func_comments = cfunc_pairs[function_name]\n",
    "    param_description_map = extract_parameters_from_func_description(func_comments)\n",
    "    param_location_map = {}\n",
    "    \n",
    "    if DEBUG:\n",
    "        print('Parameters for function: {}'.format(function_name))\n",
    "    for param_name in param_description_map:\n",
    "        param_description = param_description_map[param_name]\n",
    "        if DEBUG:\n",
    "            print(param_name, ':', param_description)\n",
    "            \n",
    "        \n",
    "    return param_location_map\n",
    "    \n",
    "def get_unlabeled_csv(json_filepath):\n",
    "    DEBUG = False\n",
    "    raw_filepath = json_filepath.replace('.json', '.py')\n",
    "    comment_list = []\n",
    "    with open(os.path.join('./raw', raw_filepath), 'rb') as f:\n",
    "        for tok in tokenize.tokenize(f.readline):\n",
    "            if tok.type == 3:\n",
    "                comment_list.append((tok.start[0], tok.end[0], tok.string))\n",
    "                \n",
    "    with open(os.path.join('./index', json_filepath), 'r') as f:\n",
    "        file_content = json.loads(f.read())\n",
    "\n",
    "    imports = ' '.join([a for a in file_content['Import']])\n",
    "    ln_fdef = {}\n",
    "    function_params = {}\n",
    "    for fd in file_content['FunctionDef']:\n",
    "        for ln in file_content['FunctionDef'][fd]['lineno']:\n",
    "            if ln not in ln_fdef:\n",
    "                ln_fdef[ln] = []\n",
    "            ln_fdef[ln].append(fd)\n",
    "        function_params[fd] = file_content['FunctionDef'][fd]['params']\n",
    "        \n",
    "    cfunc_pairs = {}\n",
    "    for clns, clne, cs in comment_list:\n",
    "        if clns-1 in ln_fdef:\n",
    "            for f in ln_fdef[clns-1]:\n",
    "                cfunc_pairs[f] = cs\n",
    "        if clne+1 in ln_fdef:\n",
    "            for f in ln_fdef[clne+1]:\n",
    "                cfunc_pairs[f] = cs\n",
    "                \n",
    "    if DEBUG:\n",
    "        for func, comment in cfunc_pairs.items():\n",
    "            print(func)\n",
    "            print(function_params[func])\n",
    "            print(comment)\n",
    "        print('\\n')\n",
    "        \n",
    "    func_comments = [cfunc_pairs[x] for x in cfunc_pairs.keys()]\n",
    "    param_description_maps = [extract_parameters_from_func_description(x) for x in func_comments]\n",
    "    \n",
    "    ret = []\n",
    "    \n",
    "    \n",
    "    for param_description_map in param_description_maps:\n",
    "        for param_name, comment in param_description_map.items():\n",
    "            temp_text = param_name + ':' + param_description_map[param_name] + '\\n'\n",
    "            if ':' in temp_text:\n",
    "                ret.append((param_name, param_description_map[param_name], imports))\n",
    "                \n",
    "    return ret\n",
    "    \n",
    "def get_unlabeled_data_across_all_files(root_dir, file_limit = 10):\n",
    "    ret = []\n",
    "    count = 0\n",
    "    for root, dirs, files in os.walk(os.path.join('index', root_dir)):\n",
    "        root = f'{os.sep}'.join(root.split(os.sep)[1:])\n",
    "        for file in files:\n",
    "            if file.endswith('.json'):\n",
    "                count += 1\n",
    "                ret += get_unlabeled_csv(os.path.join(root, file))\n",
    "                if count >= file_limit and file_limit != -1:\n",
    "                    break\n",
    "                \n",
    "    with open('data.csv', 'w') as file:\n",
    "        file.write('parameter,comment\\n')\n",
    "        for param_name, comment, imports in ret:\n",
    "            file.write(param_name + ',' + comment + ','+ imports + '\\n')\n",
    "            \n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[49], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m get_unlabeled_data_across_all_files(\u001b[39m'\u001b[39;49m\u001b[39mscikit-learn\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[48], line 132\u001b[0m, in \u001b[0;36mget_unlabeled_data_across_all_files\u001b[0;34m(root_dir, file_limit)\u001b[0m\n\u001b[1;32m    130\u001b[0m ret \u001b[39m=\u001b[39m []\n\u001b[1;32m    131\u001b[0m count \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m--> 132\u001b[0m \u001b[39mfor\u001b[39;00m root, dirs, files \u001b[39min\u001b[39;00m os\u001b[39m.\u001b[39mwalk(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\u001b[39m'\u001b[39m\u001b[39mindex\u001b[39m\u001b[39m'\u001b[39m, root_dir)):\n\u001b[1;32m    133\u001b[0m     root \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mos\u001b[39m.\u001b[39msep\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(root\u001b[39m.\u001b[39msplit(os\u001b[39m.\u001b[39msep)[\u001b[39m1\u001b[39m:])\n\u001b[1;32m    134\u001b[0m     \u001b[39mfor\u001b[39;00m file \u001b[39min\u001b[39;00m files:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/os.py:418\u001b[0m, in \u001b[0;36m_walk\u001b[0;34m(top, topdown, onerror, followlinks)\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[39m# Issue #23605: os.path.islink() is used instead of caching\u001b[39;00m\n\u001b[1;32m    414\u001b[0m         \u001b[39m# entry.is_symlink() result during the loop on os.scandir() because\u001b[39;00m\n\u001b[1;32m    415\u001b[0m         \u001b[39m# the caller can replace the directory entry during the \"yield\"\u001b[39;00m\n\u001b[1;32m    416\u001b[0m         \u001b[39m# above.\u001b[39;00m\n\u001b[1;32m    417\u001b[0m         \u001b[39mif\u001b[39;00m followlinks \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m islink(new_path):\n\u001b[0;32m--> 418\u001b[0m             \u001b[39myield from\u001b[39;00m _walk(new_path, topdown, onerror, followlinks)\n\u001b[1;32m    419\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    420\u001b[0m     \u001b[39m# Recurse into sub-directories\u001b[39;00m\n\u001b[1;32m    421\u001b[0m     \u001b[39mfor\u001b[39;00m new_path \u001b[39min\u001b[39;00m walk_dirs:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/os.py:417\u001b[0m, in \u001b[0;36m_walk\u001b[0;34m(top, topdown, onerror, followlinks)\u001b[0m\n\u001b[1;32m    412\u001b[0m         new_path \u001b[39m=\u001b[39m join(top, dirname)\n\u001b[1;32m    413\u001b[0m         \u001b[39m# Issue #23605: os.path.islink() is used instead of caching\u001b[39;00m\n\u001b[1;32m    414\u001b[0m         \u001b[39m# entry.is_symlink() result during the loop on os.scandir() because\u001b[39;00m\n\u001b[1;32m    415\u001b[0m         \u001b[39m# the caller can replace the directory entry during the \"yield\"\u001b[39;00m\n\u001b[1;32m    416\u001b[0m         \u001b[39m# above.\u001b[39;00m\n\u001b[0;32m--> 417\u001b[0m         \u001b[39mif\u001b[39;00m followlinks \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m islink(new_path):\n\u001b[1;32m    418\u001b[0m             \u001b[39myield from\u001b[39;00m _walk(new_path, topdown, onerror, followlinks)\n\u001b[1;32m    419\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    420\u001b[0m     \u001b[39m# Recurse into sub-directories\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/posixpath.py:167\u001b[0m, in \u001b[0;36mislink\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Test whether a path is a symbolic link\"\"\"\u001b[39;00m\n\u001b[1;32m    166\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 167\u001b[0m     st \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39;49mlstat(path)\n\u001b[1;32m    168\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mOSError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m, \u001b[39mAttributeError\u001b[39;00m):\n\u001b[1;32m    169\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "get_unlabeled_data_across_all_files('scikit-learn', -1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "feature set: parameters, comments, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_input_format(parameter, comment, options, labels):\n",
    "    return (' '.join([parameter, comment, options]), labels)\n",
    "\n",
    "def extract_csv_to_numpy_array(path):\n",
    "    np_array = []\n",
    "    with open(path, 'r') as file:\n",
    "        for line in file.readlines()[1:]:\n",
    "            line = line.strip().split(',')\n",
    "            dvsl = convert_to_input_format(line[0], line[1], line[2], line[3])\n",
    "            if dvsl[-1] != '' and ' ' not in dvsl[-1]:\n",
    "                np_array.append(dvsl)\n",
    "    \n",
    "    np_array = np.array(np_array)\n",
    "\n",
    "    return np_array[:,0], np_array[:,1]\n",
    "                    \n",
    "def convert_labels_to_discrete(labels: np.ndarray):\n",
    "    le = LabelEncoder()\n",
    "    return le.fit_transform(labels), le\n",
    "\n",
    "def get_bow_model(data: np.ndarray):\n",
    "    count_vec = CountVectorizer()\n",
    "    bow = count_vec.fit_transform(data)\n",
    "    bow = np.array(bow.todense())\n",
    "    return bow, count_vec\n",
    "\n",
    "def get_dense_vect_for_single_str(item, count_vec):\n",
    "    return count_vec.transform([item]).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, labels = extract_csv_to_numpy_array('data.csv')\n",
    "bow, count_vec = get_bow_model(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels, le= convert_labels_to_discrete(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB()"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_classifier = GaussianNB()\n",
    "nb_classifier.fit(X=bow, y=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9612068965517241"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(nb_classifier.predict(bow) == labels) / len(bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'docutils'"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le.inverse_transform(nb_classifier.predict([bow[0]]))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assess_comment(comment, model, le, count_vec):\n",
    "    item = comment\n",
    "    return le.inverse_transform(model.predict(get_dense_vect_for_single_str(item, count_vec)))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ggharibian/miniconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:593: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'numpy'"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assess_comment('The number of samples in the data is 1000', nb_classifier, le, count_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def chirag_comparison(comment):\n",
    "#     if 'array-like' in comment or 'ndarray' in comment or 'RandomState' in comment:\n",
    "#         return 'numpy'\n",
    "#     else:\n",
    "#         return 'builtin'\n",
    "\n",
    "# sum(le.transform([chirag_comparison(x) for x in data]) == labels) / len(labels)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e32f703e4b4aeee2270df2522a490ac6a60a6bb0e1bf2eab687b01239260a1c9"
  },
  "kernelspec": {
   "display_name": "Python 3.9.16 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
